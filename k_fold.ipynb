{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k-fold.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyPvYmsw46IVw3ahelf9pgHu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/100jy/voice_competition/blob/master/k_fold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2EMwEQ6btf_",
        "colab_type": "text"
      },
      "source": [
        "# 드라이브 연동 및 모듈 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DJFtcnt-8fz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "14e267ad-2cdf-4b04-dc02-1179f47158fb"
      },
      "source": [
        "#구글 드라이브 연동\n",
        "# 클라우드 권한 획득\n",
        "from google.colab import auth, drive\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from scipy.io import wavfile\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, Callback\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4deWmFxFbx23",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 로드 및 TPU 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZpe_U6b_fEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_batch(size=1600,seed=100): \n",
        "  GCS_PATH = 'gs://data_bucket_9586/' \n",
        "  AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
        "\n",
        "  def read_train(example):\n",
        "    feature = {'data': tf.io.VarLenFeature(dtype = tf.float32),\n",
        "              'label': tf.io.VarLenFeature(dtype = tf.float32),\n",
        "              }\n",
        "    example = tf.io.parse_single_example(example, feature)\n",
        "\n",
        "    data = tf.sparse.to_dense(example['data'])\n",
        "    label = tf.sparse.to_dense(example['label'])\n",
        "\n",
        "    data = tf.reshape(data,(299,299,1))\n",
        "    label = tf.reshape(label,(30,1))\n",
        "\n",
        "    return data, label\n",
        "\n",
        "  filenames = tf.io.gfile.glob(GCS_PATH + 'train_resize_*.tfrec')\n",
        "  spit_len = int(len(filenames) * 0.8)\n",
        "\n",
        "  np.random.seed(seed)\n",
        "  np.random.shuffle(filenames)\n",
        "\n",
        "  global train_dataset\n",
        "  train_dataset = tf.data.TFRecordDataset(filenames[:spit_len],num_parallel_reads=AUTO)\n",
        "  #batch_size = 3200\n",
        "  train_dataset = train_dataset.map(read_train, num_parallel_calls=AUTO).batch(size).prefetch(1)\n",
        "\n",
        "  global val_dataset\n",
        "  val_dataset = tf.data.TFRecordDataset(filenames[spit_len:],num_parallel_reads=AUTO)\n",
        "  #batch_size = 3200\n",
        "  val_dataset = val_dataset.map(read_train, num_parallel_calls=AUTO).batch(size).prefetch(1)  \n",
        "\n",
        "  for record in train_dataset.take(1):\n",
        "    print(len(record))\n",
        "    input_shape = (record[0].shape[1],record[0].shape[2],record[0].shape[3])\n",
        "    output_shape = record[1].shape[1]\n",
        "\n",
        "  print(input_shape)\n",
        "  print(output_shape)\n",
        "\n",
        "  # Detect hardware\n",
        "  try:\n",
        "    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "  except ValueError:\n",
        "    tpu_resolver = None\n",
        "    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "\n",
        "  # Select appropriate distribution strategy\n",
        "  if tpu_resolver:\n",
        "    tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu_resolver)\n",
        "\n",
        "  elif len(gpus) > 1:\n",
        "    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "\n",
        "  elif len(gpus) == 1:\n",
        "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "\n",
        "  else:\n",
        "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "\n",
        "  return strategy, train_dataset, val_dataset"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUuOX4HUb8tl",
        "colab_type": "text"
      },
      "source": [
        "# 모델 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDnOQ3hXcCbn",
        "colab_type": "text"
      },
      "source": [
        "## Inception net like CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dvqUF1om_nvf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inception_model_2(input_shape):\n",
        "  def inception_module(x, o_1=64,r_3 =64, o_3 =128,r_5=16,o_5=32,pool=32):\n",
        "    #size_1 filter\n",
        "    x_1 = Conv2D(o_1,1,padding='same',kernel_regularizer=l2(1e-5))(x)\n",
        "\n",
        "    #size_1 + size_5 filter\n",
        "    x_3 = Conv2D(r_5,1,padding='same',kernel_regularizer=l2(1e-5))(x)\n",
        "    x_3 = Conv2D(o_5,5,padding='same',kernel_regularizer=l2(1e-5))(x_3)\n",
        "\n",
        "    #pooling\n",
        "    x_4 = MaxPooling2D(pool_size = (3,3),strides =1,padding='same')(x)\n",
        "    x_4 = Conv2D(pool, 1, padding='same',kernel_regularizer=l2(1e-5))(x_4)\n",
        "\n",
        "    return concatenate([x_1,x_3,x_4])\n",
        "\n",
        "  #DeepCNN with Gelu\n",
        "  def gelu(x):\n",
        "    return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
        "\n",
        "\n",
        "  inp = Input(input_shape) \n",
        "\n",
        "  x = Conv2D(64, (7, 7), strides = 2, padding = \"same\", kernel_initializer='he_normal')(inp)\n",
        "  x = Activation(gelu)(x)\n",
        "  x = MaxPooling2D((3, 3), padding = \"same\", strides = 2)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Conv2D(64, (1, 1), strides = 1, padding = \"same\", kernel_initializer='he_normal')(x)\n",
        "  x = Activation(gelu)(x)\n",
        "  x = Conv2D(192, (3, 3), strides = 1, padding = \"same\", kernel_initializer='he_normal')(x)\n",
        "  x = Activation(gelu)(x)\n",
        "  x = MaxPooling2D((3, 3), padding = \"same\", strides = 2)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = inception_module(x, o_1=64, r_3=64, o_3=128, r_5=16, o_5=32, pool=32)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=128, r_3=128, o_3=192, r_5=32, o_5=96, pool=64)\n",
        "  x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=192, r_3=96, o_3=208, r_5=16, o_5=48, pool=64)\n",
        "  x = BatchNormalization()(x)\n",
        "  \n",
        "  x = inception_module(x, o_1=160, r_3=112, o_3=224, r_5=24, o_5=64, pool=64)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=128, r_3=128, o_3=256, r_5=24, o_5=64, pool=64)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=112, r_3=144, o_3=288, r_5=32, o_5=64, pool=64)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=256, r_3=160, o_3=320, r_5=32, o_5=128, pool=128)\n",
        "\n",
        "  x = MaxPooling2D(pool_size=(2, 2), strides=1, padding='same')(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=256, r_3=160, o_3=320, r_5=32, o_5=128, pool=128)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=384, r_3=192, o_3=384, r_5=48, o_5=128, pool=128)\n",
        "  \n",
        "\n",
        "  x = AveragePooling2D(pool_size=(2, 2), strides=3)(x)\n",
        "  x = Conv2D(128, (1, 1),padding = \"same\",kernel_regularizer=l2(1e-5))(x)\n",
        "  x = Activation(gelu)(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dropout(0.8)(x)\n",
        "\n",
        "\n",
        "  x = Dense(1024)(x)\n",
        "  x = Activation(gelu)(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Dropout(0.8)(x)\n",
        "\n",
        "\n",
        "  output = Dense(30, activation = \"softmax\")(x)\n",
        "\n",
        "  return Model(inp,output)\n"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "codwx0LWRc-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inception_model_1(input_shape):\n",
        "  def inception_module(x, o_1=64,r_3 =64, o_3 =128,r_5=16,o_5=32,pool=32):\n",
        "    #size_1 filter\n",
        "    x_1 = Conv2D(o_1,1,padding='same',kernel_regularizer=l2(1e-5))(x)\n",
        "\n",
        "    #size_1 + size_5 filter\n",
        "    x_3 = Conv2D(r_5,1,padding='same',kernel_regularizer=l2(1e-5))(x)\n",
        "    x_3 = Conv2D(o_5,5,padding='same',kernel_regularizer=l2(1e-5))(x_3)\n",
        "\n",
        "    #pooling\n",
        "    x_4 = MaxPooling2D(pool_size = (3,3),strides =1,padding='same')(x)\n",
        "    x_4 = Conv2D(pool, 1, padding='same',kernel_regularizer=l2(1e-5))(x_4)\n",
        "\n",
        "    return concatenate([x_1,x_3,x_4])\n",
        "\n",
        "  #DeepCNN with Gelu\n",
        "  def gelu(x):\n",
        "      return 0.5*x*(1+tf.tanh(np.sqrt(2/np.pi)*(x+0.044715*tf.pow(x, 3))))\n",
        "\n",
        "\n",
        "  inp = Input(input_shape) \n",
        "\n",
        "  x = Conv2D(64, (7, 7), strides = 2, padding = \"same\",kernel_regularizer=l2(1e-5))(inp)\n",
        "  x = Activation(gelu)(x)\n",
        "  x = MaxPooling2D((3, 3), padding = \"same\", strides = 2)(x)\n",
        "  x = Conv2D(64, (1, 1), strides = 1, padding = \"same\",kernel_regularizer=l2(1e-5))(x)\n",
        "  x = Activation(gelu)(x)\n",
        "  x = Conv2D(192, (3, 3), strides = 1, padding = \"same\",kernel_regularizer=l2(1e-5))(x)\n",
        "  x = Activation(gelu)(x)\n",
        "  x = MaxPooling2D((3, 3), padding = \"same\", strides = 2)(x)\n",
        "\n",
        "  x = inception_module(x, o_1=64, r_3=64, o_3=128, r_5=16, o_5=32, pool=32)\n",
        "  x = inception_module(x, o_1=128, r_3=128, o_3=192, r_5=32, o_5=96, pool=64)\n",
        "  x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
        "  x = inception_module(x, o_1=192, r_3=96, o_3=208, r_5=16, o_5=48, pool=64)\n",
        "  \n",
        "  x = inception_module(x, o_1=160, r_3=112, o_3=224, r_5=24, o_5=64, pool=64)\n",
        "  x = inception_module(x, o_1=128, r_3=128, o_3=256, r_5=24, o_5=64, pool=64)\n",
        "  x = inception_module(x, o_1=112, r_3=144, o_3=288, r_5=32, o_5=64, pool=64)\n",
        "  x = inception_module(x, o_1=256, r_3=160, o_3=320, r_5=32, o_5=128, pool=128)\n",
        "\n",
        "  x = MaxPooling2D(pool_size=(2, 2), strides=1, padding='same')(x)\n",
        "  x = inception_module(x, o_1=256, r_3=160, o_3=320, r_5=32, o_5=128, pool=128)\n",
        "  x = inception_module(x, o_1=384, r_3=192, o_3=384, r_5=48, o_5=128, pool=128)\n",
        "  \n",
        "\n",
        "  x = AveragePooling2D(pool_size=(2, 2), strides=3)(x)\n",
        "  x = Conv2D(128, (1, 1),padding = \"same\",kernel_regularizer=l2(1e-5))(x)\n",
        "  x = Activation(gelu)(x)\n",
        "  x = Flatten()(x)\n",
        "  #x = Dropout(0.8)(x)\n",
        "\n",
        "  '''\n",
        "  x = Dense(1024)(x)\n",
        "  x = Activation(gelu)(x)\n",
        "  x = Dropout(0.8)(x)\n",
        "  '''\n",
        "\n",
        "  output = Dense(30, activation = \"softmax\")(x)\n",
        "\n",
        "  return Model(inp,output)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XhwePXOcNSo",
        "colab_type": "text"
      },
      "source": [
        "## SE_ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIBX5IPPZRxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def conv2d_bn(x, filters, kernel_size, padding='same', strides=1, activation='relu'):\n",
        "    x = Conv2D(filters, kernel_size, kernel_initializer='he_normal', padding=padding, strides=strides)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if activation:\n",
        "        x = Activation(activation)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def SE_block(input_tensor, reduction_ratio=16):\n",
        "    ch_input = K.int_shape(input_tensor)[-1]\n",
        "    ch_reduced = ch_input//reduction_ratio\n",
        "    \n",
        "    # Squeeze\n",
        "    x = GlobalAveragePooling2D()(input_tensor) # Eqn.2\n",
        "    \n",
        "    # Excitation\n",
        "    x = Dense(ch_reduced, kernel_initializer='he_normal', activation='relu', use_bias=False)(x) # Eqn.3\n",
        "    x = Dense(ch_input, kernel_initializer='he_normal', activation='sigmoid', use_bias=False)(x) # Eqn.3\n",
        "    \n",
        "    x = Reshape( (1, 1, ch_input) )(x)\n",
        "    x = Multiply()([input_tensor, x]) # Eqn.4\n",
        "    \n",
        "    return x\n",
        "   \n",
        "\n",
        "def SE_residual_block(input_tensor, filter_sizes, strides=1, reduction_ratio=16):\n",
        "    filter_1, filter_2, filter_3 = filter_sizes\n",
        "    \n",
        "    x = conv2d_bn(input_tensor, filter_1, (1, 1), strides=strides)\n",
        "    x = conv2d_bn(x, filter_2, (3, 3))\n",
        "    x = conv2d_bn(x, filter_3, (1, 1), activation=None)\n",
        "    \n",
        "    x = SE_block(x, reduction_ratio)\n",
        "    \n",
        "    projected_input = conv2d_bn(input_tensor, filter_3, (1, 1), strides=strides, activation=None) if K.int_shape(input_tensor)[-1] != filter_3 else input_tensor\n",
        "    shortcut = Add()([projected_input, x])\n",
        "    shortcut = Activation(activation='relu')(shortcut)\n",
        "    \n",
        "    return shortcut\n",
        " \n",
        "\n",
        "def stage_block(input_tensor, filter_sizes, blocks, reduction_ratio=16, stage=''):\n",
        "    strides = 2 if stage != '2' else 1\n",
        "    \n",
        "    x = SE_residual_block(input_tensor, filter_sizes, strides, reduction_ratio) # projection layer\n",
        "\n",
        "    for i in range(blocks-1):\n",
        "        x = SE_residual_block(x, filter_sizes, reduction_ratio=reduction_ratio)\n",
        "    \n",
        "    return x\n",
        "    \n",
        "\n",
        "def SE_ResNet50(input_shape, classes=30):\n",
        "    model_input = Input(input_shape)\n",
        "    stage_1 = conv2d_bn(model_input, 64, (7, 7), strides=2, padding='same') # (112, 112, 64)\n",
        "    stage_1 = MaxPooling2D((3, 3), strides=2, padding='same')(stage_1) # (56, 56, 64)\n",
        "    \n",
        "    stage_2 = stage_block(stage_1, [64, 64, 256], 3, reduction_ratio=16, stage='2')\n",
        "    stage_3 = stage_block(stage_2, [128, 128, 512], 4, reduction_ratio=16, stage='3') # (28, 28, 512)\n",
        "    stage_4 = stage_block(stage_3, [256, 256, 1024], 6, reduction_ratio=16, stage='4') # (14, 14, 1024)\n",
        "    stage_5 = stage_block(stage_4, [512, 512, 2048], 3, reduction_ratio=16, stage='5') # (7, 7, 2048)\n",
        "\n",
        "    gap = GlobalAveragePooling2D()(stage_5)\n",
        "    \n",
        "    model_output = Dense(classes, activation='softmax', kernel_initializer='he_normal')(gap) # 'softmax'\n",
        "    \n",
        "    model = Model(inputs=model_input, outputs=model_output, name='SE-ResNet50')\n",
        "        \n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUr8Z9VTUg9m",
        "colab_type": "text"
      },
      "source": [
        "## DenseNet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G4z4HWVx7QF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv_Block(x, growth_rate, activation='relu'):\n",
        "    x_l = BatchNormalization()(x)\n",
        "    x_l = Activation(activation)(x_l)\n",
        "    x_l = Conv2D(growth_rate*4, (1, 1), padding='same', kernel_initializer='he_normal')(x_l)\n",
        "    \n",
        "    x_l = BatchNormalization()(x_l)\n",
        "    x_l = Activation(activation)(x_l)\n",
        "    x_l = Conv2D(growth_rate, (3, 3), padding='same', kernel_initializer='he_normal')(x_l)\n",
        "    \n",
        "    x = Concatenate()([x, x_l])\n",
        "    \n",
        "    return x\n",
        "\n",
        "def Dense_Block(x, layers, growth_rate=32):\n",
        "    for i in range(layers):\n",
        "        x = Conv_Block(x, growth_rate)\n",
        "    return x\n",
        "\n",
        "def Transition_Layer(x, compression_factor=0.5, activation='relu'):\n",
        "    reduced_filters = int(K.int_shape(x)[-1] * compression_factor)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activation)(x)\n",
        "    x = Conv2D(reduced_filters, (1, 1), padding='same', kernel_initializer='he_normal')(x)\n",
        "    \n",
        "    x = AveragePooling2D((2, 2), padding='same', strides=2)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "layers_in_block = {'DenseNet-121' : [6, 12, 24, 16],\n",
        "                   'DenseNet-169' : [6, 12, 32, 32],\n",
        "                   'DenseNet-201' : [6, 12, 48, 32],\n",
        "                   'DenseNet-265' : [6, 12, 64, 48]}\n",
        "\n",
        "base_growth_rate = 32\n",
        "\n",
        "def DenseNet(model_input, classes, densenet_type='DenseNet-121'):\n",
        "    model_input = Input(model_input)\n",
        "    x = Conv2D(base_growth_rate*2, (7, 7), padding='same', strides=2, kernel_initializer='he_normal')(model_input) # (224, 224, 3) -> (112, 112, 64)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    x = MaxPooling2D((3, 3), padding='same', strides=2)(x) # (112, 112, 64) -> (56, 56, 64)\n",
        "    \n",
        "    x = Dense_Block(x, layers_in_block[densenet_type][0], base_growth_rate)\n",
        "    x = Transition_Layer(x, compression_factor=0.5)\n",
        "    x = Dense_Block(x, layers_in_block[densenet_type][1], base_growth_rate)\n",
        "    x = Transition_Layer(x, compression_factor=0.5)\n",
        "    x = Dense_Block(x, layers_in_block[densenet_type][2], base_growth_rate)\n",
        "    x = Transition_Layer(x, compression_factor=0.5)\n",
        "    x = Dense_Block(x, layers_in_block[densenet_type][3], base_growth_rate)\n",
        "    \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    \n",
        "    model_output = Dense(classes, activation='softmax', kernel_initializer='he_normal')(x)\n",
        "\n",
        "    model = Model(model_input, model_output, name=densenet_type)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANsuGu7cc88",
        "colab_type": "text"
      },
      "source": [
        "# 필요 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZz8kvVW_s6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model(id, n_epoch,strategy, train_dataset, val_dataset, road_weight =False,model_name ='m1',optimizer_set='radam'):\n",
        "  model_path = 'drive/My Drive/data/model_{}_{}/'.format(model_name,id)\n",
        " \n",
        "  if not os.path.exists(model_path):\n",
        "    os.mkdir(model_path)\n",
        "      \n",
        "  # validattion 기준 모델 갱신\n",
        "  model_file = model_path + 'epoch_{epoch:03d}_val_{val_loss:3f}.hdf5'\n",
        "  checkpoint = ModelCheckpoint(filepath = model_file, monitor = 'val_loss', verbose = 1, save_best_only =True)\n",
        "\n",
        "  if optimizer_set == 'radam':\n",
        "    radam = tfa.optimizers.RectifiedAdam()\n",
        "    optimizer  = tfa.optimizers.Lookahead(radam, sync_period=3, slow_step_size=0.5)\n",
        "\n",
        "  elif optimizer_set == 'lamb':\n",
        "    ramb = tfa.optimizers.LAMB()\n",
        "    optimizer  = tfa.optimizers.Lookahead(ramb, sync_period=6, slow_step_size=0.5)\n",
        "\n",
        "\n",
        "  with strategy.scope():\n",
        "    if model_name == 'inception_1':\n",
        "      model = inception_model_1((299,299,1))\n",
        "    elif model_name == 'inception_2':\n",
        "      model = inception_model_2((299,299,1))\n",
        "    elif model_name == 'SE_ResNet':\n",
        "      model = SE_ResNet50((299,299,1),30)\n",
        "    elif model_name == 'DenseNet':\n",
        "      model = DenseNet((299,299,1),30)\n",
        "    model.compile(loss=tf.keras.losses.KLDivergence(), optimizer = optimizer)\n",
        "    if road_weight:\n",
        "      model.load_weights(glob('drive/My Drive/data/model_high_resol_{}/*.hdf5'.format(id))[-1])\n",
        "    history = model.fit(train_dataset, epochs = n_epoch, validation_data=val_dataset, callbacks = [checkpoint])\n",
        "  \n",
        "def fit_k_models(k = 5,n_epoch=6, model_name='SE_ResNet',batch_size = 64,large_batch=False):\n",
        "  for i in range(1,k+1):\n",
        "    strategy, train_dataset, val_dataset = set_batch(batch_size,i)\n",
        "    if large_batch:\n",
        "      fit_model(id=i, n_epoch=n_epoch, strategy=strategy, train_dataset=train_dataset, val_dataset=val_dataset, road_weight=False ,model_name=model_name,optimizer_set='lamb')\n",
        "    else:\n",
        "      fit_model(id=i, n_epoch=n_epoch, strategy=strategy, train_dataset=train_dataset, val_dataset=val_dataset, road_weight=False ,model_name=model_name)\n",
        "\n",
        "def load_test_set():\n",
        "  GCS_PATH = 'gs://data_bucket_9586/' \n",
        "  AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
        "\n",
        "  def read_test(example):\n",
        "    features = {'data': tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "\n",
        "    data = tf.sparse.to_dense(example['data'])\n",
        "\n",
        "    data = tf.reshape(data,(299,299,1))\n",
        "    return data\n",
        "    \n",
        "  filenames = tf.io.gfile.glob(GCS_PATH + 'test_*.tfrec')\n",
        "  test_dataset = tf.data.TFRecordDataset(filenames)\n",
        "  test_dataset = test_dataset.map(read_test).batch(128).prefetch(1)\n",
        "\n",
        "  return test_dataset\n",
        "\n",
        "def make_result(model_num,test_dataset,model_name):\n",
        "  # 가장 좋은 모델의 weight를 불러옵니다.\n",
        "  input_shape = (299,299,1)\n",
        "  model = SE_ResNet50((299,299,1),30)\n",
        "  weight_file = glob('drive/My Drive/data/model_{}_{}/*.hdf5'.format(model_name,model_num))[-1]\n",
        "  print(weight_file)\n",
        "  model.load_weights(weight_file)\n",
        "\n",
        "  # 예측 수행\n",
        "  y_pred = model.predict(test_dataset)\n",
        "\n",
        "  return y_pred\n",
        "\n",
        "def k_fold(k, model_name='SE_ResNet',batch_size = 64,n_epoch = 6, train_model=True,large_batch=False):\n",
        "  #data load\n",
        "  test_dataset = load_test_set()\n",
        "\n",
        "  #model_fit\n",
        "  if train_model:\n",
        "    if large_batch:\n",
        "      fit_k_models(k = k,n_epoch = n_epoch, model_name=model_name,batch_size = batch_size,large_batch=True)\n",
        "    else:\n",
        "      fit_k_models(k = k,n_epoch = n_epoch, model_name=model_name,batch_size = batch_size)\n",
        "\n",
        "  #make_result\n",
        "  submission = pd.read_csv('drive/My Drive/data/submission.csv', index_col=0)\n",
        "  suma = np.zeros_like(submission)\n",
        "  for i in range(1,k+1):\n",
        "    suma += make_result(i,test_dataset,model_name=model_name)\n",
        "  submission.loc[:, :] = suma / 5\n",
        "  submission.to_csv('drive/My Drive/data/submission.csv')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWxohMbnchSm",
        "colab_type": "text"
      },
      "source": [
        "# k-folds로 결과 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JblZ5dinaXkO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 107
        },
        "outputId": "c97d9b43-c891-43d9-c4aa-8f3759740f73"
      },
      "source": [
        "k_fold(5, model_name='SE_ResNet', batch_size = 64, n_epoch=12,train_model=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/data/model_high_resol_1/epoch_012_val_0.423293.hdf5\n",
            "drive/My Drive/data/model_high_resol_2/epoch_012_val_0.391400.hdf5\n",
            "drive/My Drive/data/model_high_resol_3/epoch_011_val_0.396777.hdf5\n",
            "drive/My Drive/data/model_high_resol_4/epoch_012_val_0.375129.hdf5\n",
            "drive/My Drive/data/model_high_resol_5/epoch_012_val_0.369588.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iDZyoAbxBQ6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "k_fold(1, model_name='DenseNet', batch_size = 128, n_epoch=2,large_batch=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VweVMP9M0W0K",
        "colab_type": "text"
      },
      "source": [
        "# CV-Stacking "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwNxmqFK0WQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cv_stacking(nets=5):\n",
        "  #데이터 로드\n",
        "  X = pd.read_pickle('drive/My Drive/data/stacked_x').values\n",
        "  predict_test = pd.read_pickle('drive/My Drive/data/stacked_x_test').values\n",
        "  GCS_PATH = 'gs://data_bucket_9586/' \n",
        "  \n",
        "  def read_label(example):\n",
        "    features = {'label': tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "    example = tf.io.parse_single_example(example, features)\n",
        "\n",
        "    data = tf.sparse.to_dense(example['label'])\n",
        "\n",
        "    data = tf.reshape(data,(30,))\n",
        "    return data\n",
        "\n",
        "  filenames = tf.io.gfile.glob(GCS_PATH + 'train_resize_*.tfrec')\n",
        "  np.random.seed(1)\n",
        "  np.random.shuffle(filenames) \n",
        "\n",
        "  spit_len = int(len(filenames) * 0.0334)   \n",
        "  filenames = filenames[:spit_len]\n",
        "  \n",
        "  label = tf.data.TFRecordDataset(filenames)\n",
        "  label = label.map(read_label)\n",
        "  max_elems = np.iinfo(np.int64).max\n",
        "  dataset = label.batch(max_elems)\n",
        "  label= tf.data.experimental.get_single_element(dataset).numpy()\n",
        "\n",
        "  #신경망 학습\n",
        "\n",
        "  submission = pd.read_csv('drive/My Drive/data/submission.csv', index_col=0)\n",
        "  suma = np.zeros_like(submission) \n",
        "\n",
        "  for t in range(nets):\n",
        "    np.random.seed(t)\n",
        "    np.random.shuffle(X) \n",
        "    np.random.seed(t)\n",
        "    np.random.shuffle(label) \n",
        "\n",
        "    spit_len = int(len(X) * 0.9)   \n",
        "    X_train = X[:spit_len]\n",
        "    X_val = X[spit_len:]\n",
        "\n",
        "    label_train = label[:spit_len]\n",
        "    label_val = label[spit_len:]\n",
        "\n",
        "    def simple_net(input_shape):\n",
        "      inp = Input(input_shape)\n",
        "      x = Dense(150)(inp)\n",
        "      x = BatchNormalization()(x)\n",
        "      x = Activation('relu')(x)\n",
        "\n",
        "      for i in range(2):\n",
        "        x = Dense(150)(x)\n",
        "        x = BatchNormalization()(x)\n",
        "        x = Activation('relu')(x)\n",
        "\n",
        "      out = Dense(30,activation='softmax')(x)\n",
        "      return Model(inp,out)\n",
        "\n",
        "    model = simple_net((150,))\n",
        "    model.compile(loss=tf.keras.losses.KLDivergence(), optimizer = 'adam')\n",
        "    model.fit(X_train, label_train,epochs=15, batch_size=16,validation_data=(X_val, label_val))\n",
        "    suma += model.predict(predict_test)\n",
        "\n",
        "  #결과 만들기\n",
        "  submission.loc[:, :] = suma / nets\n",
        "  submission.to_csv('drive/My Drive/data/submission_stack.csv')\n",
        "  print('submission_stack.csv is updated!')"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-40JmVaGPDUp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " cv_stacking(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVryT0V8YNFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "d2353bef-974d-417b-8a7e-8c13214d0638"
      },
      "source": [
        "def make_stacked_data():\n",
        "  def load_data(seed=1):\n",
        "    GCS_PATH = 'gs://data_bucket_9586/' \n",
        "    AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
        "\n",
        "    def read_data(example):\n",
        "      features = {'data': tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "      example = tf.io.parse_single_example(example, features)\n",
        "\n",
        "      data = tf.sparse.to_dense(example['data'])\n",
        "\n",
        "      data = tf.reshape(data,(299,299,1))\n",
        "      return data\n",
        "      \n",
        "    def read_label(example):\n",
        "      features = {'label': tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "      example = tf.io.parse_single_example(example, features)\n",
        "\n",
        "      data = tf.sparse.to_dense(example['label'])\n",
        "\n",
        "      data = tf.reshape(data,(30,))\n",
        "      return data\n",
        "\n",
        "    filenames = tf.io.gfile.glob(GCS_PATH + 'train_resize_*.tfrec')\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(filenames) \n",
        "\n",
        "    #전체에 대해\n",
        "    spit_len = int(len(filenames) * 0.0334)   \n",
        "    filenames = filenames[:spit_len]\n",
        "    \n",
        "\n",
        "    data = tf.data.TFRecordDataset(filenames)\n",
        "    data = data.map(read_data).batch(128).prefetch(1)\n",
        "\n",
        "    label = tf.data.TFRecordDataset(filenames)\n",
        "    label = label.map(read_label)\n",
        "      \n",
        "    return data,label\n",
        "\n",
        "  data,label = load_data()\n",
        "  #test_dataset = load_test_set()\n",
        "    \n",
        "  print('loading is complete!')\n",
        "\n",
        "  for i in range(1,6):      \n",
        "      #일부에 대한 출력을 만듬\n",
        "    new = make_result(i, data, 'high_resol')\n",
        "    #new_test = make_result(i, test_dataset, 'high_resol')\n",
        "    print('prediction by m{} is done'.format(i))\n",
        "    if i > 1 :\n",
        "      #해당 결과들 stacking\n",
        "      predict = np.concatenate((predict,new), axis=1)\n",
        "      #predict_test =  np.concatenate((predict_test,new_test), axis=1)\n",
        "    else:\n",
        "      predict = new\n",
        "      #predict_test = new_test\n",
        "\n",
        "  print('predict is done!')\n",
        "\n",
        "  pd.DataFrame(predict).to_pickle('drive/My Drive/data/stacked_x')\n",
        "  #pd.DataFrame(predict_test).to_pickle('drive/My Drive/data/stacked_x_test')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  make_stacked_data()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading is complete!\n",
            "drive/My Drive/data/model_high_resol_1/epoch_012_val_0.423293.hdf5\n",
            "prediction by m1 is done\n",
            "drive/My Drive/data/model_high_resol_2/epoch_012_val_0.391400.hdf5\n",
            "prediction by m2 is done\n",
            "drive/My Drive/data/model_high_resol_3/epoch_011_val_0.396777.hdf5\n",
            "prediction by m3 is done\n",
            "drive/My Drive/data/model_high_resol_4/epoch_012_val_0.375129.hdf5\n",
            "prediction by m4 is done\n",
            "drive/My Drive/data/model_high_resol_5/epoch_012_val_0.369588.hdf5\n",
            "prediction by m5 is done\n",
            "predict is done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Ac45IosAaf1",
        "colab_type": "text"
      },
      "source": [
        "# 다른 모델 찾아서 stacking and l2로 LightGBM 이용(효과좋을듯)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKB3a_aY8qn",
        "colab_type": "text"
      },
      "source": [
        "# 8/4\n",
        "--------------------------------------\n",
        "\n",
        "**모델** : SE_ResNet50(5-folds,6 epochs)\n",
        "  \n",
        "**데이터**: x_trian_mel-spectogram(299*299)\n",
        "  \n",
        "**score** : **0.72265 (15위)**\n",
        "\n",
        "**메모** : shifted aug not good, k-folds good, 에폭 더 주면 많이 좋을 듯..\n",
        "\n",
        "**피드백** : 12에폭으로 결과 확인\n",
        "\n",
        "--------------------------------------\n",
        "\n",
        "# 8/5\n",
        "--------------------------------------\n",
        "\n",
        "**모델** : SE_ResNet50(5-folds,12 epochs)\n",
        "  \n",
        "**데이터**: x_trian_mel-spectogram(299*299)\n",
        "  \n",
        "**score** : **0.71xx (16위)**\n",
        "\n",
        "**메모** : 0.7대에서 성능 개선 없음, 드랍아웃 적용하고 에폭더 주면 좋을듯 \n",
        "\n",
        "**피드백** : 드랍아웃 추가 3-fold, 20epoch\n",
        "\n",
        "--------------------------------------\n"
      ]
    }
  ]
}