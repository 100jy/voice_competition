{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "k-fold.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm",
      "authorship_tag": "ABX9TyME4dQHyjzgt9vlT+t7cgrw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/100jy/voice_competition/blob/master/k_fold.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2EMwEQ6btf_",
        "colab_type": "text"
      },
      "source": [
        "# 드라이브 연동 및 모듈 임포트"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3DJFtcnt-8fz",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "942ba5c2-ae02-4e2b-87a9-033c15f680a5"
      },
      "source": [
        "#구글 드라이브 연동\n",
        "# 클라우드 권한 획득\n",
        "from google.colab import auth, drive\n",
        "auth.authenticate_user()\n",
        "drive.mount('/content/drive')\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy\n",
        "import tensorflow as tf\n",
        "import keras\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "from scipy.io import wavfile\n",
        "from tensorflow.keras.models import Sequential,Model\n",
        "from tensorflow.keras.layers import *\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping, CSVLogger, Callback\n",
        "from tensorflow.keras.optimizers import *\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.regularizers import l2\n",
        "import tensorflow_addons as tfa"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4deWmFxFbx23",
        "colab_type": "text"
      },
      "source": [
        "# 데이터 로드 및 TPU 초기화"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZpe_U6b_fEL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def set_batch(size=1600,seed=100,dim=2): \n",
        "  GCS_PATH = 'gs://data_bucket_9586/' \n",
        "  AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
        "\n",
        "  if dim == 2:\n",
        "    def read_train(example):\n",
        "      feature = {'data': tf.io.VarLenFeature(dtype = tf.float32),\n",
        "                'label': tf.io.VarLenFeature(dtype = tf.float32),\n",
        "                }\n",
        "      example = tf.io.parse_single_example(example, feature)\n",
        "\n",
        "      data = tf.sparse.to_dense(example['data'])\n",
        "      label = tf.sparse.to_dense(example['label'])\n",
        "\n",
        "      data = tf.reshape(data,(299,299,1))\n",
        "      label = tf.reshape(label,(30,1))\n",
        "\n",
        "      return data, label\n",
        "\n",
        "    filenames = tf.io.gfile.glob(GCS_PATH + 'train_resize_*.tfrec')\n",
        "    #반만 보기...\n",
        "    spit_len = int(len(filenames) * 0.9)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(filenames)\n",
        "\n",
        "    train_dataset = tf.data.TFRecordDataset(filenames[:spit_len],num_parallel_reads=AUTO)\n",
        "    #batch_size = 3200\n",
        "    train_dataset = train_dataset.map(read_train, num_parallel_calls=AUTO).batch(size).prefetch(1)\n",
        "\n",
        "    val_dataset = tf.data.TFRecordDataset(filenames[spit_len:spit_len+1],num_parallel_reads=AUTO)\n",
        "    #batch_size = 3200\n",
        "    val_dataset = val_dataset.map(read_train, num_parallel_calls=AUTO).batch(size).prefetch(1)  \n",
        "\n",
        "    for record in train_dataset.take(1):\n",
        "      print(len(record))\n",
        "      input_shape = (record[0].shape[1],record[0].shape[2],record[0].shape[3])\n",
        "      output_shape = record[1].shape[1]\n",
        "\n",
        "    print(input_shape)\n",
        "    print(output_shape)\n",
        "  \n",
        "  else:\n",
        "    GCS_PATH = 'gs://data_bucket_9586/' \n",
        "    AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
        "\n",
        "    def read_train(example):\n",
        "      feature = {'data': tf.io.VarLenFeature(dtype = tf.float32),\n",
        "                'label': tf.io.VarLenFeature(dtype = tf.float32),\n",
        "                }\n",
        "      example = tf.io.parse_single_example(example, feature)\n",
        "\n",
        "      data = tf.sparse.to_dense(example['data'])\n",
        "      label = tf.sparse.to_dense(example['label'])\n",
        "\n",
        "      data = tf.reshape(data,(11025,1))\n",
        "      label = tf.reshape(label,(30,1))\n",
        "\n",
        "      return data, label\n",
        "\n",
        "    filenames = tf.io.gfile.glob(GCS_PATH + 'train_wave_*.tfrec')\n",
        "    spit_len = int(len(filenames) * 0.8)\n",
        "\n",
        "    np.random.seed(seed)\n",
        "    np.random.shuffle(filenames)\n",
        "\n",
        "    train_dataset = tf.data.TFRecordDataset(filenames[:spit_len],num_parallel_reads=AUTO)\n",
        "    #batch_size = 3200\n",
        "    train_dataset = train_dataset.map(read_train, num_parallel_calls=AUTO).batch(size).prefetch(1)\n",
        "\n",
        "    val_dataset = tf.data.TFRecordDataset(filenames[spit_len:],num_parallel_reads=AUTO)\n",
        "    #batch_size = 3200\n",
        "    val_dataset = val_dataset.map(read_train, num_parallel_calls=AUTO).batch(size).prefetch(1)  \n",
        "\n",
        "    for record in train_dataset.take(1):\n",
        "      print(len(record))\n",
        "      input_shape = (record[0].shape[1])\n",
        "      output_shape = record[1].shape[1]\n",
        "\n",
        "    print(input_shape)\n",
        "    print(output_shape)\n",
        "\n",
        "  # Detect hardware\n",
        "  try:\n",
        "    tpu_resolver = tf.distribute.cluster_resolver.TPUClusterResolver() # TPU detection\n",
        "  except ValueError:\n",
        "    tpu_resolver = None\n",
        "    gpus = tf.config.experimental.list_logical_devices(\"GPU\")\n",
        "\n",
        "  # Select appropriate distribution strategy\n",
        "  if tpu_resolver:\n",
        "    tf.config.experimental_connect_to_cluster(tpu_resolver)\n",
        "    tf.tpu.experimental.initialize_tpu_system(tpu_resolver)\n",
        "    strategy = tf.distribute.experimental.TPUStrategy(tpu_resolver)\n",
        "\n",
        "  elif len(gpus) > 1:\n",
        "    strategy = tf.distribute.MirroredStrategy([gpu.name for gpu in gpus])\n",
        "\n",
        "  elif len(gpus) == 1:\n",
        "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "\n",
        "  else:\n",
        "    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n",
        "\n",
        "  return strategy, train_dataset, val_dataset"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BUuOX4HUb8tl",
        "colab_type": "text"
      },
      "source": [
        "# 모델 정의"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WVgDfFwtiQEz",
        "colab_type": "text"
      },
      "source": [
        "# 1D CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oHp007WjiSeq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def conv1d_bn(x, filters, kernel_size, padding='same', strides=1, activation='relu'):\n",
        "    x = Conv1D(filters, kernel_size, kernel_initializer='he_normal', padding=padding, strides=strides)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if activation:\n",
        "        x = Activation(activation)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "def filter_module(x,filters,weight_decay=1e-4):\n",
        "    x_1 = conv1d_bn(x,filters=filters, kernel_size=3, padding='same')\n",
        "    x_1 = MaxPooling1D(3, padding = \"same\", strides = 3)(x_1)\n",
        "\n",
        "    x_3 = conv1d_bn(x,filters=filters, kernel_size=5, padding='same')\n",
        "    x_3 = MaxPooling1D(3, padding = \"same\", strides = 3)(x_3)\n",
        "\n",
        "    x_4 = conv1d_bn(x,filters=filters, kernel_size=1, padding='same')\n",
        "    x_4 = MaxPooling1D(3, padding = \"same\", strides = 3)(x_4)\n",
        "   \n",
        "    return concatenate([x_1,x_3,x_4])\n",
        "\n",
        "def wave_model(input_shape):\n",
        "  inputs = Input(input_shape)\n",
        "  x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(inputs)\n",
        "  #First Conv1D layer\n",
        "  x = Conv1D(8,13, padding='valid', activation='relu', strides=1)(x)\n",
        "  x = MaxPooling1D(3)(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  #Second Conv1D layer\n",
        "  x = Conv1D(16, 11, padding='valid', activation='relu', strides=1)(x)\n",
        "  x = MaxPooling1D(3)(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  #Third Conv1D layer\n",
        "  x = Conv1D(32, 9, padding='valid', activation='relu', strides=1)(x)\n",
        "  x = MaxPooling1D(3)(x)\n",
        "  x = Dropout(0.3)(x)\n",
        "  x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(x)\n",
        "  x = Bidirectional(GRU(128, return_sequences=True), merge_mode='sum')(x)\n",
        "  x = Bidirectional(GRU(128, return_sequences=True), merge_mode='sum')(x)\n",
        "  x = Bidirectional(GRU(128, return_sequences=False), merge_mode='sum')(x)\n",
        "  x = BatchNormalization(axis=-1, momentum=0.99, epsilon=1e-3, center=True, scale=True)(x)\n",
        "  #Flatten layer\n",
        "  # x = Flatten()(x)\n",
        "  #Dense Layer 1\n",
        "  x = Dense(256, activation='relu')(x)\n",
        "  outputs = Dense(30, activation=\"softmax\")(x)\n",
        "\n",
        "  return Model(inputs,outputs)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sDnOQ3hXcCbn",
        "colab_type": "text"
      },
      "source": [
        "## InceptionNet like CNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "codwx0LWRc-l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def inception_model_1(input_shape):\n",
        "  def inception_module(x, o_1=64,r_3 =64, o_3 =128,r_5=16,o_5=32,pool=32):\n",
        "    #size_1 filter\n",
        "    x_1 = Conv2D(o_1,1,padding='same')(x)\n",
        "\n",
        "    #size_1 + size_5 filter\n",
        "    x_3 = Conv2D(r_5,1,padding='same')(x)\n",
        "    x_3 = Conv2D(o_5,5,padding='same')(x_3)\n",
        "\n",
        "    #pooling\n",
        "    x_4 = MaxPooling2D(pool_size = (3,3),strides =1,padding='same')(x)\n",
        "    x_4 = Conv2D(pool, 1, padding='same')(x_4)\n",
        "\n",
        "    return concatenate([x_1,x_3,x_4])\n",
        "\n",
        "\n",
        "\n",
        "  inp = Input(input_shape) \n",
        "\n",
        "  x = Conv2D(64, (7, 7), strides = 2, padding = \"same\")(inp)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D((3, 3), padding = \"same\", strides = 2)(x)\n",
        "  x = Conv2D(64, (1, 1), strides = 1, padding = \"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Conv2D(192, (3, 3), strides = 1, padding = \"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = MaxPooling2D((3, 3), padding = \"same\", strides = 2)(x)\n",
        "\n",
        "  x = inception_module(x, o_1=64, r_3=64, o_3=128, r_5=16, o_5=32, pool=32)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=128, r_3=128, o_3=192, r_5=32, o_5=96, pool=64)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = MaxPooling2D(pool_size=(3, 3), strides=2, padding='same')(x)\n",
        "  x = inception_module(x, o_1=192, r_3=96, o_3=208, r_5=16, o_5=48, pool=64)\n",
        "  x = BatchNormalization()(x)\n",
        "  \n",
        "  x = inception_module(x, o_1=160, r_3=112, o_3=224, r_5=24, o_5=64, pool=64)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=128, r_3=128, o_3=256, r_5=24, o_5=64, pool=64)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=112, r_3=144, o_3=288, r_5=32, o_5=64, pool=64)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=256, r_3=160, o_3=320, r_5=32, o_5=128, pool=128)\n",
        "  x = BatchNormalization()(x)\n",
        "\n",
        "  x = MaxPooling2D(pool_size=(2, 2), strides=1, padding='same')(x)\n",
        "  x = inception_module(x, o_1=256, r_3=160, o_3=320, r_5=32, o_5=128, pool=128)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = inception_module(x, o_1=384, r_3=192, o_3=384, r_5=48, o_5=128, pool=128)\n",
        "  x = BatchNormalization()(x)\n",
        "  \n",
        "\n",
        "  x = AveragePooling2D(pool_size=(2, 2), strides=3)(x)\n",
        "  x = Conv2D(128, (1, 1),padding = \"same\")(x)\n",
        "  x = BatchNormalization()(x)\n",
        "  x = Activation('relu')(x)\n",
        "  x = Flatten()(x)\n",
        "  x = Dropout(0.5)(x)\n",
        "\n",
        "  output = Dense(30, activation = \"softmax\")(x)\n",
        "\n",
        "  return Model(inp,output)\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_XhwePXOcNSo",
        "colab_type": "text"
      },
      "source": [
        "## SE_ResNet"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SIBX5IPPZRxd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def conv2d_bn(x, filters, kernel_size, padding='same', strides=1, activation='relu'):\n",
        "    x = Conv2D(filters, kernel_size, kernel_initializer='he_normal', padding=padding, strides=strides)(x)\n",
        "    x = BatchNormalization()(x)\n",
        "    if activation:\n",
        "        x = Activation(activation)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "\n",
        "def SE_block(input_tensor, reduction_ratio=16):\n",
        "    ch_input = K.int_shape(input_tensor)[-1]\n",
        "    ch_reduced = ch_input//reduction_ratio\n",
        "    \n",
        "    # Squeeze\n",
        "    x = GlobalAveragePooling2D()(input_tensor) # Eqn.2\n",
        "    \n",
        "    # Excitation\n",
        "    x = Dense(ch_reduced, kernel_initializer='he_normal', activation='relu', use_bias=False)(x) # Eqn.3\n",
        "    x = Dense(ch_input, kernel_initializer='he_normal', activation='sigmoid', use_bias=False)(x) # Eqn.3\n",
        "    \n",
        "    x = Reshape( (1, 1, ch_input) )(x)\n",
        "    x = Multiply()([input_tensor, x]) # Eqn.4\n",
        "    \n",
        "    return x\n",
        "   \n",
        "\n",
        "def SE_residual_block(input_tensor, filter_sizes, strides=1, reduction_ratio=16):\n",
        "    filter_1, filter_2, filter_3 = filter_sizes\n",
        "    \n",
        "    x = conv2d_bn(input_tensor, filter_1, (1, 1), strides=strides)\n",
        "    x = conv2d_bn(x, filter_2, (3, 3))\n",
        "    x = conv2d_bn(x, filter_3, (1, 1), activation=None)\n",
        "    \n",
        "    x = SE_block(x, reduction_ratio)\n",
        "    \n",
        "    projected_input = conv2d_bn(input_tensor, filter_3, (1, 1), strides=strides, activation=None) if K.int_shape(input_tensor)[-1] != filter_3 else input_tensor\n",
        "    shortcut = Add()([projected_input, x])\n",
        "    shortcut = Activation(activation='relu')(shortcut)\n",
        "    \n",
        "    return shortcut\n",
        " \n",
        "\n",
        "def stage_block(input_tensor, filter_sizes, blocks, reduction_ratio=16, stage=''):\n",
        "    strides = 2 if stage != '2' else 1\n",
        "    \n",
        "    x = SE_residual_block(input_tensor, filter_sizes, strides, reduction_ratio) # projection layer\n",
        "\n",
        "    for i in range(blocks-1):\n",
        "        x = SE_residual_block(x, filter_sizes, reduction_ratio=reduction_ratio)\n",
        "    \n",
        "    return x\n",
        "    \n",
        "\n",
        "def SE_ResNet50(input_shape, classes=30):\n",
        "    model_input = Input(input_shape)\n",
        "    stage_1 = conv2d_bn(model_input, 64, (7, 7), strides=2, padding='same') # (112, 112, 64)\n",
        "    stage_1 = MaxPooling2D((3, 3), strides=2, padding='same')(stage_1) # (56, 56, 64)\n",
        "    \n",
        "    stage_2 = stage_block(stage_1, [64, 64, 256], 3, reduction_ratio=16, stage='2')\n",
        "    stage_3 = stage_block(stage_2, [128, 128, 512], 4, reduction_ratio=16, stage='3') # (28, 28, 512)\n",
        "    stage_4 = stage_block(stage_3, [256, 256, 1024], 6, reduction_ratio=16, stage='4') # (14, 14, 1024)\n",
        "    stage_5 = stage_block(stage_4, [512, 512, 2048], 3, reduction_ratio=16, stage='5') # (7, 7, 2048)\n",
        "\n",
        "    gap = GlobalAveragePooling2D()(stage_5)\n",
        "    x = Dropout(0.8)(gap)\n",
        "    \n",
        "    model_output = Dense(classes, activation='softmax', kernel_initializer='he_normal')(x) # 'softmax'\n",
        "    \n",
        "    model = Model(inputs=model_input, outputs=model_output, name='SE-ResNet50')\n",
        "        \n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZUr8Z9VTUg9m",
        "colab_type": "text"
      },
      "source": [
        "## DenseNet\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0G4z4HWVx7QF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def Conv_Block(x, growth_rate, activation='relu'):\n",
        "    x_l = BatchNormalization()(x)\n",
        "    x_l = Activation(activation)(x_l)\n",
        "    x_l = Conv2D(growth_rate*4, (1, 1), padding='same', kernel_initializer='he_normal')(x_l)\n",
        "    \n",
        "    x_l = BatchNormalization()(x_l)\n",
        "    x_l = Activation(activation)(x_l)\n",
        "    x_l = Conv2D(growth_rate, (3, 3), padding='same', kernel_initializer='he_normal')(x_l)\n",
        "    \n",
        "    x = Concatenate()([x, x_l])\n",
        "    \n",
        "    return x\n",
        "\n",
        "def Dense_Block(x, layers, growth_rate=32):\n",
        "    for i in range(layers):\n",
        "        x = Conv_Block(x, growth_rate)\n",
        "    return x\n",
        "\n",
        "def Transition_Layer(x, compression_factor=0.5, activation='relu'):\n",
        "    reduced_filters = int(K.int_shape(x)[-1] * compression_factor)\n",
        "    \n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation(activation)(x)\n",
        "    x = Conv2D(reduced_filters, (1, 1), padding='same', kernel_initializer='he_normal')(x)\n",
        "    \n",
        "    x = AveragePooling2D((2, 2), padding='same', strides=2)(x)\n",
        "    \n",
        "    return x\n",
        "\n",
        "layers_in_block = {'DenseNet-121' : [6, 12, 24, 16],\n",
        "                   'DenseNet-169' : [6, 12, 32, 32],\n",
        "                   'DenseNet-201' : [6, 12, 48, 32],\n",
        "                   'DenseNet-265' : [6, 12, 64, 48]}\n",
        "\n",
        "base_growth_rate = 32\n",
        "\n",
        "def DenseNet(model_input, classes, densenet_type='DenseNet-121'):\n",
        "    model_input = Input(model_input)\n",
        "    x = Conv2D(base_growth_rate*2, (7, 7), padding='same', strides=2, kernel_initializer='he_normal')(model_input) # (224, 224, 3) -> (112, 112, 64)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    \n",
        "    x = MaxPooling2D((3, 3), padding='same', strides=2)(x) # (112, 112, 64) -> (56, 56, 64)\n",
        "    \n",
        "    x = Dense_Block(x, layers_in_block[densenet_type][0], base_growth_rate)\n",
        "    x = Transition_Layer(x, compression_factor=0.5)\n",
        "    x = Dense_Block(x, layers_in_block[densenet_type][1], base_growth_rate)\n",
        "    x = Transition_Layer(x, compression_factor=0.5)\n",
        "    x = Dense_Block(x, layers_in_block[densenet_type][2], base_growth_rate)\n",
        "    x = Transition_Layer(x, compression_factor=0.5)\n",
        "    x = Dense_Block(x, layers_in_block[densenet_type][3], base_growth_rate)\n",
        "    \n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "    \n",
        "    model_output = Dense(classes, activation='softmax', kernel_initializer='he_normal')(x)\n",
        "\n",
        "    model = Model(model_input, model_output, name=densenet_type)\n",
        "    \n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ANsuGu7cc88",
        "colab_type": "text"
      },
      "source": [
        "# 필요 함수 정의"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pZz8kvVW_s6P",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def fit_model(id, n_epoch,strategy, train_dataset, val_dataset, road_weight =False,model_name ='m1',optimizer_set='radam'):\n",
        "  model_path = 'drive/My Drive/data/model_{}_{}/'.format(model_name,id)\n",
        " \n",
        "  if not os.path.exists(model_path):\n",
        "    os.mkdir(model_path)\n",
        "      \n",
        "  # validattion 기준 모델 갱신\n",
        "  model_file = model_path + 'epoch_{epoch:03d}_val_{val_loss:3f}.hdf5'\n",
        "  checkpoint = ModelCheckpoint(filepath = model_file, monitor = 'val_loss', verbose = 1, save_best_only =True)\n",
        "\n",
        "  if optimizer_set == 'radam':\n",
        "    radam = tfa.optimizers.RectifiedAdam()\n",
        "    optimizer  = tfa.optimizers.Lookahead(radam, sync_period=3, slow_step_size=0.5)\n",
        "\n",
        "  elif optimizer_set == 'lamb':\n",
        "    ramb = tfa.optimizers.LAMB()\n",
        "    optimizer  = tfa.optimizers.Lookahead(ramb, sync_period=3, slow_step_size=0.5)\n",
        "\n",
        "\n",
        "  with strategy.scope():\n",
        "    if model_name == 'inception_1':\n",
        "      model = inception_model_1((299,299,1))\n",
        "    elif model_name == 'SE_ResNet':\n",
        "      model = SE_ResNet50((299,299,1),30)\n",
        "    elif model_name == 'DenseNet':\n",
        "      model = DenseNet((299,299,1),30)\n",
        "    elif model_name == '1D_model':\n",
        "      model = wave_model((11025,1))\n",
        "    model.compile(loss=tf.keras.losses.KLDivergence(), optimizer = optimizer)\n",
        "    if road_weight:\n",
        "      model.load_weights(glob('drive/My Drive/data/model_high_resol_{}/*.hdf5'.format(id))[-1])\n",
        "    history = model.fit(train_dataset, epochs = n_epoch, validation_data=val_dataset, callbacks = [checkpoint])\n",
        "  \n",
        "def fit_k_models(k = 5,n_epoch=6, model_name='SE_ResNet',batch_size = 64,large_batch=False,dim=2):\n",
        "  for i in range(16,k+1):\n",
        "    if dim == 2:\n",
        "      strategy, train_dataset, val_dataset = set_batch(batch_size,i)\n",
        "      if large_batch:\n",
        "        fit_model(id=i, n_epoch=n_epoch, strategy=strategy, train_dataset=train_dataset, val_dataset=val_dataset, road_weight=False ,model_name=model_name,optimizer_set='lamb')\n",
        "      else:\n",
        "        fit_model(id=i, n_epoch=n_epoch, strategy=strategy, train_dataset=train_dataset, val_dataset=val_dataset, road_weight=False ,model_name=model_name)\n",
        "    else:\n",
        "      strategy, train_dataset, val_dataset = set_batch(batch_size,i,dim=1)\n",
        "      if large_batch:\n",
        "        fit_model(id=i, n_epoch=n_epoch, strategy=strategy, train_dataset=train_dataset, val_dataset=val_dataset, road_weight=False ,model_name=model_name,optimizer_set='lamb')\n",
        "      else:\n",
        "        fit_model(id=i, n_epoch=n_epoch, strategy=strategy, train_dataset=train_dataset, val_dataset=val_dataset, road_weight=False ,model_name=model_name)\n",
        "\n",
        "\n",
        "def load_test_set(dim=2):\n",
        "  \n",
        "  GCS_PATH = 'gs://data_bucket_9586/' \n",
        "  AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
        "\n",
        "  if dim == 1:\n",
        "    def read_test(example):\n",
        "      features = {'data': tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "      example = tf.io.parse_single_example(example, features)\n",
        "\n",
        "      data = tf.sparse.to_dense(example['data'])\n",
        "\n",
        "      data = tf.reshape(data,(11025,1))\n",
        "      return data\n",
        "      \n",
        "    filenames = tf.io.gfile.glob(GCS_PATH + 'test_wave*.tfrec')\n",
        "    test_dataset = tf.data.TFRecordDataset(filenames)\n",
        "    test_dataset = test_dataset.map(read_test).batch(512).prefetch(1)\n",
        "\n",
        "  else:\n",
        "    def read_test_2d(example):\n",
        "      features = {'data': tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "      example = tf.io.parse_single_example(example, features)\n",
        "\n",
        "      data = tf.sparse.to_dense(example['data'])\n",
        "\n",
        "      data = tf.reshape(data,(299,299,1))\n",
        "      return data\n",
        "      \n",
        "    filenames = tf.io.gfile.glob(GCS_PATH + 'test_*.tfrec')[:10]\n",
        "    test_dataset = tf.data.TFRecordDataset(filenames)\n",
        "    test_dataset = test_dataset.map(read_test_2d).batch(128).prefetch(1)\n",
        "\n",
        "  return test_dataset\n",
        "\n",
        "def make_result(model_num,test_dataset,model_name):\n",
        "  # 가장 좋은 모델의 weight를 불러옵니다.\n",
        "  if model_name == 'inception_1':\n",
        "    model = inception_model_1((299,299,1))\n",
        "  elif model_name == 'SE_ResNet':\n",
        "    model = SE_ResNet50((299,299,1),30)\n",
        "  elif model_name == 'DenseNet':\n",
        "    model = DenseNet((299,299,1),30)\n",
        "  elif model_name == '1D_model':\n",
        "    model = wave_model((11025,1))\n",
        "\n",
        "  weight_file = glob('drive/My Drive/data/model_{}_{}/*.hdf5'.format(model_name,model_num))[-1]\n",
        "  print(weight_file)\n",
        "  model.load_weights(weight_file)\n",
        "  # 예측 수행\n",
        "  y_pred = model.predict(test_dataset)\n",
        "\n",
        "  return y_pred\n",
        "\n",
        "def k_fold(k, model_name='SE_ResNet',batch_size = 64,n_epoch = 6, train_model=True,large_batch=False,only_train=False,oneDim=False):\n",
        "  if oneDim:\n",
        "    test_dataset = load_test_set(dim=1)\n",
        "  else:   \n",
        "    #data load\n",
        "    test_dataset = load_test_set()\n",
        "\n",
        "  #model_fit\n",
        "  if train_model:\n",
        "    if large_batch: \n",
        "      if oneDim:\n",
        "        fit_k_models(k = k,n_epoch = n_epoch, model_name=model_name,batch_size = batch_size,large_batch=True,dim=1)\n",
        "      else:\n",
        "        fit_k_models(k = k,n_epoch = n_epoch, model_name=model_name,batch_size = batch_size,large_batch=True)\n",
        "    else:\n",
        "      if oneDim:\n",
        "        fit_k_models(k = k,n_epoch = n_epoch, model_name=model_name,batch_size = batch_size,dim=1)\n",
        "      else:\n",
        "        fit_k_models(k = k,n_epoch = n_epoch, model_name=model_name,batch_size = batch_size)\n",
        "\n",
        "  if not(only_train):\n",
        "    #make_result\n",
        "    submission = pd.read_csv('drive/My Drive/data/submission.csv', index_col=0)\n",
        "    suma = np.zeros_like(submission)\n",
        "    for i in range(1,k+1):\n",
        "      suma += make_result(i,test_dataset,model_name=model_name)\n",
        "    submission.loc[:, :] = suma / k\n",
        "    submission.to_csv('drive/My Drive/data/submission.csv')\n",
        "    print('submission.csv is updated!')"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fWxohMbnchSm",
        "colab_type": "text"
      },
      "source": [
        "# k-folds로 결과 저장"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nJ0d-SxIUOZP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "8782de53-872f-468d-8ecb-ebd54ce37b2e"
      },
      "source": [
        "k_fold(20, model_name='SE_ResNet', batch_size = 64, n_epoch=12, train_model=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2\n",
            "(299, 299, 1)\n",
            "30\n",
            "WARNING:tensorflow:TPU system grpc://10.81.229.162:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:TPU system grpc://10.81.229.162:8470 has already been initialized. Reinitializing the TPU can cause previously created variables on TPU to be lost.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.81.229.162:8470\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Initializing the TPU system: grpc://10.81.229.162:8470\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Clearing out eager caches\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Finished initializing TPU system.\n",
            "WARNING:absl:`tf.distribute.experimental.TPUStrategy` is deprecated, please use  the non experimental symbol `tf.distribute.TPUStrategy` instead.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Found TPU system:\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Workers: 1\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Num TPU Cores Per Worker: 8\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:localhost/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:CPU:0, CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:0, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:1, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:2, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:3, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:4, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:5, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:6, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU:7, TPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:TPU_SYSTEM:0, TPU_SYSTEM, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:*** Available Device: _DeviceAttributes(/job:worker/replica:0/task:0/device:XLA_CPU:0, XLA_CPU, 0, 0)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/12\n",
            "   4219/Unknown - 1278s 303ms/step - loss: 1.8262\n",
            "Epoch 00001: val_loss improved from inf to 1.37677, saving model to drive/My Drive/data/model_SE_ResNet_16/epoch_001_val_1.376767.hdf5\n",
            "4219/4219 [==============================] - 1310s 310ms/step - loss: 1.8262 - val_loss: 1.3768\n",
            "Epoch 2/12\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 1.1384\n",
            "Epoch 00002: val_loss improved from 1.37677 to 0.92934, saving model to drive/My Drive/data/model_SE_ResNet_16/epoch_002_val_0.929339.hdf5\n",
            "4219/4219 [==============================] - 661s 157ms/step - loss: 1.1384 - val_loss: 0.9293\n",
            "Epoch 3/12\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.8552\n",
            "Epoch 00003: val_loss improved from 0.92934 to 0.68039, saving model to drive/My Drive/data/model_SE_ResNet_16/epoch_003_val_0.680388.hdf5\n",
            "4219/4219 [==============================] - 656s 156ms/step - loss: 0.8552 - val_loss: 0.6804\n",
            "Epoch 4/12\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.6654\n",
            "Epoch 00004: val_loss improved from 0.68039 to 0.63677, saving model to drive/My Drive/data/model_SE_ResNet_16/epoch_004_val_0.636771.hdf5\n",
            "4219/4219 [==============================] - 671s 159ms/step - loss: 0.6654 - val_loss: 0.6368\n",
            "Epoch 5/12\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.5152\n",
            "Epoch 00005: val_loss improved from 0.63677 to 0.44002, saving model to drive/My Drive/data/model_SE_ResNet_16/epoch_005_val_0.440016.hdf5\n",
            "4219/4219 [==============================] - 673s 159ms/step - loss: 0.5152 - val_loss: 0.4400\n",
            "Epoch 6/12\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.3788\n",
            "Epoch 00006: val_loss improved from 0.44002 to 0.32115, saving model to drive/My Drive/data/model_SE_ResNet_16/epoch_006_val_0.321155.hdf5\n",
            "4219/4219 [==============================] - 658s 156ms/step - loss: 0.3788 - val_loss: 0.3212\n",
            "Epoch 7/12\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.3026\n",
            "Epoch 00007: val_loss improved from 0.32115 to 0.26997, saving model to drive/My Drive/data/model_SE_ResNet_16/epoch_007_val_0.269968.hdf5\n",
            "4219/4219 [==============================] - 660s 157ms/step - loss: 0.3026 - val_loss: 0.2700\n",
            "Epoch 8/12\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.2311\n",
            "Epoch 00008: val_loss did not improve from 0.26997\n",
            "4219/4219 [==============================] - 663s 157ms/step - loss: 0.2311 - val_loss: 0.3141\n",
            "Epoch 9/12\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.2049\n",
            "Epoch 00009: val_loss did not improve from 0.26997\n",
            "4219/4219 [==============================] - 655s 155ms/step - loss: 0.2049 - val_loss: 0.3534\n",
            "Epoch 10/12\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.1662\n",
            "Epoch 00010: val_loss improved from 0.26997 to 0.20886, saving model to drive/My Drive/data/model_SE_ResNet_16/epoch_010_val_0.208857.hdf5\n",
            "4219/4219 [==============================] - 671s 159ms/step - loss: 0.1662 - val_loss: 0.2089\n",
            "Epoch 11/12\n",
            "4219/4219 [==============================] - ETA: 0s - loss: 0.1467\n",
            "Epoch 00011: val_loss improved from 0.20886 to 0.19051, saving model to drive/My Drive/data/model_SE_ResNet_16/epoch_011_val_0.190514.hdf5\n",
            "4219/4219 [==============================] - 658s 156ms/step - loss: 0.1467 - val_loss: 0.1905\n",
            "Epoch 12/12\n",
            " 170/4219 [>.............................] - ETA: 11:06 - loss: 0.1421"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G6ItAtYOoIw5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "98f45dea-b31a-4d32-92e1-30873b4df182"
      },
      "source": [
        "k_fold(15, model_name='SE_ResNet', train_model=False)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "drive/My Drive/data/model_SE_ResNet_1/epoch_012_val_0.423293.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_2/epoch_012_val_0.391400.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_3/epoch_011_val_0.396777.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_4/epoch_012_val_0.375129.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_5/epoch_012_val_0.369588.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_6/epoch_012_val_0.292427.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_7/epoch_012_val_0.303550.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_8/epoch_012_val_0.221418.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_9/epoch_012_val_0.202112.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_10/epoch_012_val_0.236830.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_11/epoch_012_val_0.251470.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_12/epoch_012_val_0.292545.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_13/epoch_012_val_0.247642.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_14/epoch_010_val_0.264577.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_15/epoch_012_val_0.337385.hdf5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VweVMP9M0W0K",
        "colab_type": "text"
      },
      "source": [
        "# CV-Stacking "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVryT0V8YNFb",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "023a9278-07a6-4f33-8460-1e3c73512f56"
      },
      "source": [
        "def make_stacked_data(model_list):\n",
        "  \n",
        "  def load_data(seed=1,dim=2):\n",
        "    GCS_PATH = 'gs://data_bucket_9586/' \n",
        "    AUTO = tf.data.experimental.AUTOTUNE # used in tf.data.Dataset API\n",
        "    if dim == 2:\n",
        "      def read_data(example):\n",
        "        features = {'data': tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "        example = tf.io.parse_single_example(example, features)\n",
        "\n",
        "        data = tf.sparse.to_dense(example['data'])\n",
        "\n",
        "        data = tf.reshape(data,(299,299,1))\n",
        "        return data\n",
        "        \n",
        "      def read_label(example):\n",
        "        features = {'label': tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "        example = tf.io.parse_single_example(example, features)\n",
        "\n",
        "        data = tf.sparse.to_dense(example['label'])\n",
        "\n",
        "        data = tf.reshape(data,(30,))\n",
        "        return data\n",
        "\n",
        "      filenames = tf.io.gfile.glob(GCS_PATH + 'train_resize_*.tfrec')[:100]\n",
        "      np.random.seed(seed)\n",
        "      np.random.shuffle(filenames)\n",
        "      filenames = filenames[:10]\n",
        "    \n",
        "    else:\n",
        "      def read_data(example):\n",
        "        features = {'data': tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "        example = tf.io.parse_single_example(example, features)\n",
        "\n",
        "        data = tf.sparse.to_dense(example['data'])\n",
        "\n",
        "        data = tf.reshape(data,(11025,1))\n",
        "        return data\n",
        "        \n",
        "      def read_label(example):\n",
        "        features = {'label': tf.io.VarLenFeature(dtype = tf.float32)}\n",
        "        example = tf.io.parse_single_example(example, features)\n",
        "\n",
        "        data = tf.sparse.to_dense(example['label'])\n",
        "\n",
        "        data = tf.reshape(data,(30,))\n",
        "        return data\n",
        "\n",
        "      filenames = tf.io.gfile.glob(GCS_PATH + 'train_wave_*.tfrec')[:100]\n",
        "      np.random.seed(seed)\n",
        "      np.random.shuffle(filenames)\n",
        "\n",
        "\n",
        "    data = tf.data.TFRecordDataset(filenames)\n",
        "    data = data.map(read_data).batch(1024).prefetch(1)\n",
        "\n",
        "    label = tf.data.TFRecordDataset(filenames)\n",
        "    label = label.map(read_label)\n",
        "      \n",
        "    return data,label\n",
        "\n",
        "  data,label = load_data()\n",
        "  test_dataset = load_test_set()  \n",
        "\n",
        "  data_1d,label_1d = load_data(dim=1)\n",
        "  test_dataset_1d = load_test_set(dim=1)\n",
        "    \n",
        "  print('loading is complete!')\n",
        "  predict = np.array([])\n",
        "  for j in model_list:\n",
        "    for i in range(8,j[1]+1):    \n",
        "      #일부에 대한 출력을 만듬\n",
        "      if j[0] == '1D_model':\n",
        "        new = make_result(i, data_1d,  j[0])\n",
        "        new_test = make_result(i, test_dataset_1d, j[0])\n",
        "      else:\n",
        "        new = make_result(i, data,  j[0])\n",
        "        new_test = make_result(i, test_dataset, j[0])\n",
        "      print('prediction by {}_m{} is done'.format(j[0],i))\n",
        "      if predict.any() :\n",
        "        #해당 결과들 stacking\n",
        "        predict = np.concatenate((predict,new), axis=1)\n",
        "        predict_test =  np.concatenate((predict_test,new_test), axis=1)\n",
        "      else:\n",
        "        predict = new\n",
        "        predict_test = new_test\n",
        "\n",
        "  print('predict is done!')\n",
        "\n",
        "  pd.DataFrame(predict).to_pickle('drive/My Drive/data/stacked_x')\n",
        "  pd.DataFrame(predict_test).to_pickle('drive/My Drive/data/stacked_x_test')\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  model_list = [('SE_ResNet',10)]\n",
        "  make_stacked_data(model_list)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loading is complete!\n",
            "drive/My Drive/data/model_SE_ResNet_8/epoch_012_val_0.221418.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_8/epoch_012_val_0.221418.hdf5\n",
            "prediction by SE_ResNet_m8 is done\n",
            "drive/My Drive/data/model_SE_ResNet_9/epoch_012_val_0.202112.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_9/epoch_012_val_0.202112.hdf5\n",
            "prediction by SE_ResNet_m9 is done\n",
            "drive/My Drive/data/model_SE_ResNet_10/epoch_012_val_0.236830.hdf5\n",
            "drive/My Drive/data/model_SE_ResNet_10/epoch_012_val_0.236830.hdf5\n",
            "prediction by SE_ResNet_m10 is done\n",
            "predict is done!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwNxmqFK0WQ5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def cv_stacking(nets=5,total_models=5):\n",
        "  #데이터 로드\n",
        "  X = pd.read_pickle('drive/My Drive/data/stacked_x').values\n",
        "  predict_test = pd.read_pickle('drive/My Drive/data/stacked_x_test').values\n",
        "  label = pd.read_csv('drive/My Drive/data/stacked_y.csv',index_col=0).values\n",
        "  #test data\n",
        "  from sklearn.model_selection import train_test_split\n",
        "  X, X_test, label, label_test = train_test_split(X, label,test_size=0.1, shuffle=False,random_state=1004)\n",
        "\n",
        "\n",
        "  #신경망 학습\n",
        "  submission = pd.read_csv('drive/My Drive/data/submission.csv', index_col=0)\n",
        "  suma = np.zeros_like(submission)\n",
        "  test = np.zeros_like(label_test, dtype='float64')\n",
        "  nodes = total_models*30\n",
        "\n",
        "  for t in range(nets):\n",
        "    np.random.seed(t)\n",
        "    new_X = np.random.permutation(X) \n",
        "    np.random.seed(t)\n",
        "    new_label = np.random.permutation(label)\n",
        "\n",
        "    spit_len = int(len(new_X) * 0.9)   \n",
        "    X_train = new_X[:spit_len]\n",
        "    X_val = new_X[spit_len:]\n",
        "\n",
        "    label_train = new_label[:spit_len]\n",
        "    label_val = new_label[spit_len:]\n",
        "\n",
        "    def simple_net(input_shape):\n",
        "      inp = Input(input_shape)\n",
        "      out = Dense(30,activation='softmax')(inp)\n",
        "      return Model(inp,out)\n",
        "\n",
        "    model = simple_net((nodes,))\n",
        "    model.compile(loss=tf.keras.losses.KLDivergence(), optimizer = 'adam')\n",
        "    model.fit(X_train, label_train,epochs=12, batch_size=8,validation_data=(X_val, label_val))\n",
        "\n",
        "    suma += model.predict(predict_test)\n",
        "    test += model.predict(X_test)\n",
        "\n",
        "    \n",
        "\n",
        "    print(str((t+1)) + ' of ' + str(nets) + ' is done')\n",
        "\n",
        "  \n",
        "  #테스트\n",
        "  kl = tf.keras.losses.KLDivergence()\n",
        "  score = kl(label_test, test/nets).numpy()\n",
        "  print('score is '+ str(score))\n",
        "  \n",
        "  \n",
        "  #결과 만들기\n",
        "  submission.loc[:, :] = suma / nets\n",
        "  submission.to_csv('drive/My Drive/data/submission_stack.csv')\n",
        "  print('submission_stack.csv is updated!')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ueKB3a_aY8qn",
        "colab_type": "text"
      },
      "source": [
        "# 8/4\n",
        "--------------------------------------\n",
        "\n",
        "**모델** : SE_ResNet50(5-folds,6 epochs)\n",
        "  \n",
        "**데이터**: x_trian_mel-spectogram(299*299)\n",
        "  \n",
        "**score** : **0.72265 (15위)**\n",
        "\n",
        "**메모** : shifted aug not good, k-folds good, 에폭 더 주면 많이 좋을 듯..\n",
        "\n",
        "**피드백** : 12에폭으로 결과 확인\n",
        "\n",
        "--------------------------------------\n",
        "\n",
        "# 8/5\n",
        "--------------------------------------\n",
        "\n",
        "**모델** : SE_ResNet50(5-folds,12 epochs)\n",
        "  \n",
        "**데이터**: x_trian_mel-spectogram(299*299)\n",
        "  \n",
        "**score** : **0.71xx (16위)**\n",
        "\n",
        "**메모** : 0.7대에서 성능 개선 없음, 드랍아웃 적용하고 에폭더 주면 좋을듯 \n",
        "\n",
        "**피드백** : 드랍아웃 추가 3-fold, 20epoch\n",
        "\n",
        "--------------------------------------\n",
        "\n",
        "\n",
        "\n",
        "# 8/11\n",
        "--------------------------------------\n",
        "\n",
        "**모델** : SE_ResNet50(10-folds,12 epochs)\n",
        "  \n",
        "**데이터**: x_trian_mel-spectogram(299*299)\n",
        "  \n",
        "**score** : **0.62657 (13위)**\n",
        "\n",
        "**메모** : 드랍아웃 적용하고 5개 추가해서 대폭성능 개선\n",
        "\n",
        "**피드백** : 5fold 추가, L2 model 적용해보기..\n",
        "\n",
        "--------------------------------------\n",
        "\n",
        "--------------------------------------\n",
        "\n",
        "**모델** : SE_ResNet50(15-folds,12 epochs)\n",
        "  \n",
        "**데이터**: x_trian_mel-spectogram(299*299)\n",
        "  \n",
        "**score** : **0.6037 (13위)**\n",
        "\n",
        "**메모** : 성능 소폭 개선됨, epoch 늘릴시 빠르게 수렴하나 성능 저하가 의심됨..\n",
        "\n",
        "**피드백** : 20 fold로 결과 확인\n",
        "\n",
        "--------------------------------------"
      ]
    }
  ]
}